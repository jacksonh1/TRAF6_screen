import os
import sys
import pandas as pd

import json
import src.traf_pepseq_tools as traf_tools


def calc_read_fraction(c, cols):
    '''calculate readfraction for each day'''
    # I probably don't need to make all of these copies but I'm not really running out of memory
    #or running into bottlenecks
    counts = c.copy()
    T = counts[cols].sum()
    f = counts.copy()
    for i in cols:
        f[i] = counts[i]/T[i]
    return f


def filter_across_multiple_columns(df1, count_cutoff, cols):
    df = df1.copy()
    df = traf_tools.collapse_counts(df, cols)
    df = traf_tools.filter_nonsense_seqs(df)
    df = df[(df[cols] >= count_cutoff).any(1)]
    df = df.sort_values('seq')
    df = df[['seq','AA_seq']+cols]
    return df


def diff_single_day(df, col_i, col_f):
    return df[col_f] - df[col_i]


def fold_change_single_day(df, col_i, col_f):
    return df[col_f]/df[col_i]


def calc_daily_change(df, how = 'fold change'):
    '''calculate the change in column values over each day
    return a dataframe with deltas'''
    # deltas = pd.DataFrame(index=dat['AA_seq'])
    # deltas = dat.copy()
    dat = df.copy()
    dcols = sorted([i for i in dat.columns if 'day' in i])
    for i in range(len(dcols)-1):
        init = dcols[i]
        fin = dcols[i+1]
        if how == 'fold change':
            dat['fold change: '+init+' to '+fin] = fold_change_single_day(dat, init, fin)
        if how == 'difference':
            dat['difference: '+init+' to '+fin] = diff_single_day(dat, init, fin)
    dat=dat.drop(dcols, axis=1)
    return dat


def df2unique_seq_list(df):
    '''returns list of the unique strings in `df['AA_seq']`'''
    df1 = df.copy()
    seq_list = list(df1['AA_seq'].unique())
    return seq_list


def mask_frequency_by_count_cutoff(freq_df, counts_df, cols, mask_count_cutoff=20, replacement_number=0):
    f = freq_df.copy()
    c = counts_df.copy()
    # replace all of the entries with < mask_count_cutoff with `replacement_number`
    f[cols]=f[cols].mask(c[cols]<mask_count_cutoff, replacement_number)
    return f


def get_binders_from_day4_5(freq_df, counts_df, mask_count_cutoff=20, day45_cutoff=20, enrichment_cutoff=2):
    """filters sequences based on how many reads they have in day 4 and/or 5 and how many 
    days each sequence has enriched.

    Parameters
    ----------
    freq_df : _type_
        _description_
    counts_df : _type_
        _description_
    mask_count_cutoff : int, optional
        used in `mask_frequency_by_count_cutoff`, by default 20.
        mask frequencies that have < mask_count_cutoff reads that day before
        calculating the change in frequency
    day45_cutoff : int, optional
        read count cutoff for days 4 and/or 5 for a sequence to be considered a binder,
        by default `day45_cutoff`=20.
        A sequence must have >= `day45_cutoff` reads on day 4 and/or day 5 to be
        considered a potential binder. 
    enrichment_cutoff : int, optional
        only keep sequences that enriched (increased in frequency from one day
        to the next) a given number of times (>=`enrichment_cutoff`) during at least
        one of the replicate enrichment experiments
        Uses the masked frequency generated by `mask_frequency_by_count_cutoff`, 
        by default `enrichment_cutoff`=2


    Returns
    -------
    DataFrame
        filtered read count dataframe. Contains only AA sequences that have >= `day45_cutoff` reads on
        day 4 and/or 5 and enriched `enrichment_cutoff` or more times during the enrichment
    """    
    c = counts_df.copy()
    f = freq_df.copy()
    c = c.drop("pre-enrichment (MACSlib)", axis=1)
    f = f.drop("pre-enrichment (MACSlib)", axis=1)
    days1_5 = [x for x in c.columns if 'day' in x]
    f = mask_frequency_by_count_cutoff(
        f,
        c,
        days1_5,
        mask_count_cutoff=mask_count_cutoff,
        replacement_number=0
    )
    df=calc_daily_change(f, how='difference')
    ddays = [x for x in df.columns if 'day' in x]

    df['n days enriched']=(df[ddays]>0).sum(1)
    c['n days enriched']=c['seq'].map(df.set_index('seq')['n days enriched'])

    c_binders = c[(c['day_4']>=day45_cutoff) | (c['day_5']>=day45_cutoff)]

    c_binders_filtered = c_binders[c_binders['n days enriched']>=enrichment_cutoff]
    return c_binders_filtered


def driver(file, initial_count_cutoff, cols):
    c = pd.read_csv(file)
    c = filter_across_multiple_columns(c, initial_count_cutoff, cols)
    f = calc_read_fraction(c, cols)
    df = get_binders_from_day4_5(f, c, mask_count_cutoff=20, day45_cutoff=20, enrichment_cutoff=2)
    return df2unique_seq_list(df)


def main(parameter_file):
    # open parameters.json file
    with open(parameter_file) as f:
        params = json.load(f)

    table_files = params['filepaths']['merged enrichment tables']
    initial_count_cutoff = 50
    cols = params['enrichment count columns']
    output_file = os.path.join(params['filepaths']['output directory'], 'final_binder_list.txt')

    binders = []
    for file in table_files:
        binders.append(driver(file, initial_count_cutoff, cols))

    final_binders = traf_tools.union_2_lists(binders[0],binders[1])
    traf_tools.write_seqlist(final_binders, output_file)

    print(f'number of unique seqs with >=20 reads on day 4 or 5 and that enriched at least 2 of the 4 nights: {len(final_binders)}')
    print('file saved to {}'.format(output_file))

    # update parameter json file to include new files
    params['filepaths']['final binder list']=output_file
    with open(parameter_file, 'w') as f:
        json.dump(params, f, indent=4)


if __name__ == "__main__":
    main("./parameters.json")

